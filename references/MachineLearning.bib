Automatically generated by Mendeley 1.0.1
Any changes to this file will be lost if it is regenerated by Mendeley.

@inproceedings{jordan2002discriminative,
author = {Ng, Andrew Y. and Jordan, Michael I.},
booktitle = {Advances in neural information processing systems 14: proceedings of the 2002 conference},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Ng, Jordan - 2002 - On discriminative vs. generative classifiers A comparison of logistic regression and naive bayes.pdf:pdf},
organization = {MIT Press},
pages = {841},
title = {{On Discriminative vs. Generative classifiers: A comparison of logistic regression and naive Bayes}},
volume = {2},
year = {2002}
}
@article{Berry2007,
author = {Berry, M and Browne, M and Langville, a and Pauca, V and Plemmons, R},
doi = {10.1016/j.csda.2006.11.006},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Berry et al. - 2007 - Algorithms and applications for approximate nonnegative matrix factorization.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics & Data Analysis},
keywords = {conjugate gradient,constrained least squares,email surveillance,nonnegative matrix factorization,spectral data analysis,text mining},
month = sep,
number = {1},
pages = {155--173},
title = {{Algorithms and applications for approximate nonnegative matrix factorization}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167947306004191},
volume = {52},
year = {2007}
}
@article{Hoyer2004,
archivePrefix = {arXiv},
arxivId = {arXiv:cs/0408058v1},
author = {Hoyer, P.O.},
eprint = {0408058v1},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Hoyer - 2004 - Non-negative matrix factorization with sparseness constraints.pdf:pdf},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
keywords = {data-adaptive representations,non-negative matrix factorization,sparseness},
pages = {1457--1469},
primaryClass = {arXiv:cs},
publisher = {JMLR. org},
title = {{Non-negative matrix factorization with sparseness constraints}},
url = {http://portal.acm.org/citation.cfm?id=1044709&amp;dl=},
volume = {5},
year = {2004}
}
@article{Donoho2004,
author = {Donoho, David},
file = {:Users/ondrejmandula/Documents/papers/10.1.1.85.8157.pdf:pdf},
journal = {Advances in neural information processing},
title = {{When does non-negative matrix factorization give a correct decomposition into parts}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.85.8157&amp;rep=rep1&amp;type=pdf},
year = {2004}
}
@techreport{Directions,
annote = {This handout contains all you really need to know about KT in order to be able to solve problems.},
author = {Directions, True Feasible and Directions, Feasible and Qualification, Constraint},
booktitle = {ReCALL},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Directions, Directions, Qualification - Unknown - Kuhn-Tucker-Lagrange conditions basics.pdf:pdf},
number = {x},
pages = {1--3},
title = {{Kuhn-Tucker-Lagrange conditions : basics}},
volume = {1}
}
@article{Molgedey1994a,
annote = {Separationo of sources including their temporal structure. Referenced in Hyvarinen ICA book (Chapter 18, p. 343). Hyvarinen uses symetrized covariances so gets the eigenproblem straight ahead...
They estimate the number of sources by addidng more and more until one stay silent.
This is non-iterative computation!
No noise inlcuded.},
author = {Molgedey, L. and Schuster, H.G.},
file = {:Users/ondrejmandula/Documents/papers/PhysRevLett.72.3634.pdf:pdf},
issn = {1079-7114},
journal = {Physical Review Letters},
keywords = {source separation},
mendeley-tags = {source separation},
number = {23},
pages = {3634--3637},
publisher = {APS},
title = {{Separation of a mixture of independent signals using time delayed correlations}},
url = {http://link.aps.org/doi/10.1103/PhysRevLett.72.3634},
volume = {72},
year = {1994}
}
@techreport{Choi2006,
author = {Choi, Seungjin and Cruces-alavarez, Sergio and He, Zhoshui and Chen, Zhe and Hori, Gen and Washi-, Yoshikazu and Rutkowski, Tomasz and Funase, Arao and Rickard, Scott and Frein, Ruairi De and Amari, Shun-ichi and Siwek, Krzysztof and Tanaka, Toshihisa and Cruces-, Sergio and Zdunek, R and Amari, S and Separation, Signal and Kompass, R and Hori, G and Com-, Soft},
booktitle = {Matrix},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Choi et al. - 2006 - NMFLAB for Signal Processing.pdf:pdf},
pages = {32--39},
title = {{NMFLAB for Signal Processing}},
year = {2006}
}
@book{hyv√§rinen2001independent,
author = {Hyv\\"arinen, A and Karhunen, J and Oja, E},
isbn = {9780471405405},
publisher = {J. Wiley},
series = {Adaptive and learning systems for signal processing, communications, and control},
title = {{Independent component analysis}},
url = {http://books.google.com/books?id=96D0ypDwAkkC},
year = {2001}
}
@book{Cover1991,
abstract = {Following a brief introduction and overview, early chapters cover the basic algebraic relationships of entropy, relative entropy and mutual information, AEP, entropy rates of stochastics processes and data compression, duality of data compression and the growth rate of wealth. Later chapters explore Kolmogorov complexity, channel capacity, differential entropy, the capacity of the fundamental Gaussian channel, the relationship between information theory and statistics, rate distortion and network information theories. The final two chapters examine the stock market and inequalities in information theory. In many cases the authors actually describe the properties of the solutions before the presented problems.},
author = {Cover, T M and Thomas, J A},
booktitle = {Book},
chapter = {Rate Disto},
doi = {10.1177/0022219410375001},
editor = {Schilling, Donald L},
institution = {Wiley},
isbn = {0471062596},
issn = {15384780},
number = {Wiley Series in Telecommunications},
pages = {542},
pmid = {20660925},
publisher = {Wiley},
series = {Wiley Series in Telecommunications},
title = {{Elements of Information Theory}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/047174882X.fmatter/pdf},
volume = {1},
year = {1991}
}
@article{McKeown1998,
abstract = {Independent component analysis (ICA), which separates fMRI data into spatially independent patterns of activity, has recently been shown to be a suitable method for exploratory fMRI analysis. The validity of the assumptions of ICA, mainly that the underlying components are spatially independent and add linearly, was explored with a representative fMRI data set by calculating the log-likelihood of observing each voxel's time course conditioned on the ICA model. The probability of observing the time courses from white-matter voxels was higher compared to other observed brain regions. Regions containing blood vessels had the lowest probabilities. The statistical distribution of probabilities over all voxels did not resemble that expected for a small number of independent components mixed with Gaussian noise. These results suggest the ICA model may more accurately represent the data in specific regions of the brain, and that both the activity-dependent sources of blood flow and noise are non-Gaussian.},
author = {McKeown, M J and Sejnowski, T J},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/McKeown, Sejnowski - 1998 - Independent component analysis of fMRI data examining the assumptions..pdf:pdf},
issn = {1065-9471},
journal = {Human brain mapping},
keywords = {Color Perception,Color Perception: physiology,Computer Simulation,Humans,Likelihood Functions,Magnetic Resonance Imaging,Magnetic Resonance Imaging: methods,Models, Statistical,Normal Distribution},
month = jan,
number = {5-6},
pages = {368--72},
pmid = {9788074},
title = {{Independent component analysis of fMRI data: examining the assumptions.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/9788074},
volume = {6},
year = {1998}
}
@inproceedings{Buntine2004,
author = {Buntine, Wray and Jakulin, A.},
booktitle = {Proceedings of the 20th conference on Uncertainty in artificial intelligence},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Buntine - 2004 - Applying Discrete PCA in Data Analysis.pdf:pdf},
isbn = {0974903906},
pages = {59--66},
publisher = {AUAI Press},
title = {{Applying discrete PCA in data analysis}},
url = {http://portal.acm.org/citation.cfm?id=1036843.1036851},
year = {2004}
}
@incollection{NIPS2010_0687,
author = {Harmeling, Stefan and Michael, Hirsch and Schoelkopf, Bernhard},
booktitle = {Advances in Neural Information Processing Systems 23},
editor = {Lafferty, J and Williams, C K I and Shawe-Taylor, J and Zemel, R S and Culotta, A},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Harmeling, Hirsch, Sch\"{o}lkopf - Unknown - Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake.pdf:pdf},
pages = {829--837},
title = {{Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake}},
year = {2010}
}
@article{Kim2003,
abstract = {The availability of parallel, high-throughput biological experiments that simultaneously monitor thousands of cellular observables provides an opportunity for investigating cellular behavior in a highly quantitative manner at multiple levels of resolution. One challenge to more fully exploit new experimental advances is the need to develop algorithms to provide an analysis at each of the relevant levels of detail. Here, the data analysis method non-negative matrix factorization (NMF) has been applied to the analysis of gene array experiments. Whereas current algorithms identify relationships on the basis of large-scale similarity between expression patterns, NMF is a recently developed machine learning technique capable of recognizing similarity between subportions of the data corresponding to localized features in expression space. A large data set consisting of 300 genome-wide expression measurements of yeast was used as sample data to illustrate the performance of the new approach. Local features detected are shown to map well to functional cellular subsystems. Functional relationships predicted by the new analysis are compared with those predicted using standard approaches; validation using bioinformatic databases suggests predictions using the new approach may be up to twice as accurate as some conventional approaches.},
annote = {Estimation of the rank K for non negative matrix factorisation. 
        
Control experiment with SVD decomposition.
        
      },
author = {Kim, Philip M and Tidor, Bruce},
doi = {10.1101/gr.903503},
file = {:Users/ondrejmandula/Documents/papers/Genome Res.-2003-Kim-1706-18.pdf:pdf},
issn = {1088-9051},
journal = {Genome research},
keywords = {Algorithms,Computational Biology,Computational Biology: methods,Computational Biology: statistics & numerical data,Databases, Protein,Databases, Protein: statistics & numerical data,Gene Expression Profiling,Gene Expression Profiling: statistics & numerical ,Gene Expression Profiling: trends,Genes, Fungal,Genes, Fungal: genetics,Genes, Fungal: physiology,Genome, Fungal,Predictive Value of Tests,Proteome,Proteome: classification,Proteome: genetics,Proteome: physiology,Saccharomyces cerevisiae Proteins,Saccharomyces cerevisiae Proteins: classification,Saccharomyces cerevisiae Proteins: genetics,Saccharomyces cerevisiae Proteins: physiology},
month = jul,
number = {7},
pages = {1706--18},
pmid = {12840046},
title = {{Subsystem identification through dimensionality reduction of large-scale gene expression data.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=403744&tool=pmcentrez&rendertype=abstract},
volume = {13},
year = {2003}
}
@misc{Shewchuk1994,
author = {Shewchuk, J.R.},
booktitle = {Science},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Shewchuk - 1994 - An introduction to the conjugate gradient method without the agonizing pain.pdf:pdf},
keywords = {1,2,5,agonizing pain,conjugate gradient method,convergence analysis,eigen do it if,eigenvalues,i try,jacobi iterations,preconditioning,thinking with eigenvectors and},
publisher = {Citeseer},
title = {{An introduction to the conjugate gradient method without the agonizing pain}},
url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:An+Introduction+to+the+Conjugate+Gradient+Method+Without+the+Agonizing+Pain#0},
year = {1994}
}
@article{Feng2002,
author = {Feng, T and Li, SZ and Shum, HY},
doi = {10.1109/DEVLRN.2002.1011835},
file = {:Users/ondrejmandula/Documents/papers/01011835.pdf:pdf},
isbn = {0-7695-1459-6},
journal = {Development and Learning},
pages = {178--183},
publisher = {IEEE Comput. Soc},
title = {{Local non-negative matrix factorization as a visual representation}},
url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1011835 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1011835},
year = {2002}
}
@inproceedings{Korattikara2011,
author = {Korattikara, Anoop and Boyles, Levi and Welling, Max and Kim, Jingu and Park, Haesun},
booktitle = {2011 Fourteenth International Conference on Artificial Intelligence and Statistics},
file = {:Users/ondrejmandula/Documents/papers/balan11a.pdf:pdf},
keywords = {NMF,non-negative matrix factorization},
mendeley-tags = {NMF},
publisher = {AISTATS},
title = {{Statistical Optimization of Non-Negative Matrix Factorization}},
url = {http://www.ics.uci.edu/$\sim$akoratti/papers/FASTNMF_AISTATS2011.pdf},
volume = {15},
year = {2011}
}
@article{Hyvarinen1999,
abstract = {Sparse coding is a method for finding a representation of data in which each of the components of the representation is only rarely significantly active. Such a representation is closely related to redundancy reduction and independent component analysis, and has some neurophysiological plausibility. In this article, we show how sparse coding can be used for denoising. Using maximum likelihood estimation of nongaussian variables corrupted by gaussian noise, we show how to apply a soft-thresholding (shrinkage) operator on the components of sparse coding so as to reduce noise. Our method is closely related to the method of wavelet shrinkage, but it has the important benefit over wavelet methods that the representation is determined solely by the statistical properties of the data. The wavelet representation, on the other hand, relies heavily on certain mathematical properties (like self-similarity) that may be only weakly related to the properties of natural data.},
author = {Hyvarinen, a},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Hyvarinen - 1999 - Sparse code shrinkage denoising of nongaussian data by maximum likelihood estimation(2).pdf:pdf},
issn = {1530-888X},
journal = {Neural computation},
month = oct,
number = {7},
pages = {1739--68},
pmid = {10490945},
title = {{Sparse code shrinkage: denoising of nongaussian data by maximum likelihood estimation}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10490945},
volume = {11},
year = {1999}
}
@inproceedings{Harmeling2003,
author = {Harmeling, Stefan and Meinecke, Frank and M\\"uller, K.R.},
booktitle = {Proc. 4th International Symposium on Independent Component Analysis and Blind Signal Separation (ICA2003), Nara, Japan},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Harmeling, Meinecke, Muller - 2003 - Analysing ICA components by injecting noise.pdf:pdf},
publisher = {Citeseer},
title = {{Analysing ICA components by injecting noise}},
url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:ANALYSING+ICA+COMPONENTS+BY+INJECTING+NOISE#0},
volume = {1},
year = {2003}
}
@techreport{Smith1995,
author = {Smith, S M and Brady, J M},
booktitle = {Magnetic Resonance Imaging},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Smith, Brady - 1995 - SUSAN A New Approach to Low Level Image Processing 2 The SUSAN Principle for Feature Detection.pdf:pdf},
keywords = {c crown copyright 1995,defence research agency,edge detection,farnborough,feature detection,gu14 6td,hampshire,noise reduction,smoothing,uk,univalue areas},
pages = {1--59},
title = {{SUSAN A New Approach to Low Level Image Processing 2 The SUSAN Principle for Feature Detection}},
year = {1995}
}
@phdthesis{Harmeling2004,
author = {Harmeling, Stefan},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Harmeling - 2004 - Independent component analysis and beyond Dissertation.pdf:pdf},
number = {August},
title = {{Independent component analysis and beyond Dissertation}},
year = {2004}
}
@article{Minka2001,
author = {Minka, T.P.},
file = {:Users/ondrejmandula/Documents/papers/minka2001NIPS.pdf:pdf},
issn = {1049-5258},
journal = {Advances in neural information processing systems},
number = {514},
pages = {598--604},
publisher = {Citeseer},
title = {{Automatic choice of dimensionality for PCA}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.93.1144&amp;rep=rep1&amp;type=pdf},
year = {2001}
}
@article{Chow1961,
author = {Chow, Y S and Robbins, H},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Chow, Robbins - 1961 - on Sums of Independent Random Variables With Infinite Moments and Fair Games..pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
month = mar,
number = {3},
pages = {330--5},
pmid = {16590832},
title = {{on Sums of Independent Random Variables With Infinite Moments and "Fair" Games.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=223497&tool=pmcentrez&rendertype=abstract},
volume = {47},
year = {1961}
}
@article{Hyvarinen2001,
abstract = {A generalization of projection pursuit for time series, that is, signals with time structure, is introduced. The goal is to find projections of time series that have interesting structure, defined using criteria related to Kolmogoroff complexity or coding length. Interesting signals are those that can be coded with a short code length. We derive a simple approximation of coding length that takes into account both the nongaussianity and the autocorrelations of the time series. Also, we derive a simple algorithm for its approximative optimization. The resulting method is closely related to blind separation of nongaussian, time-dependent source signals.},
author = {Hyv\"{a}rinen, a},
file = {:Users/ondrejmandula/Documents/papers/Hyvarinen2001NC.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
month = apr,
number = {4},
pages = {883--98},
pmid = {11255574},
title = {{Complexity pursuit: separating interesting components from time series.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11255574},
volume = {13},
year = {2001}
}
@article{Jambunathan1954,
author = {Jambunathan, MV},
file = {:Users/ondrejmandula/Documents/papers/PropertiesOfGammaBetaDistrib.pdf:pdf},
journal = {The annals of mathematical statistics},
number = {2},
pages = {401--405},
publisher = {JSTOR},
title = {{Some Properties of Beta and Gamma Distributions}},
url = {http://www.jstor.org/stable/2236745},
volume = {25},
year = {1954}
}
@article{Cichocki,
author = {Cichocki, a. and Zdunek, R. and Amari, S.},
doi = {10.1109/ICASSP.2006.1661352},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Cichocki, Zdunek, Amari - Unknown - New Algorithms for Non-Negative Matrix Factorization in Applications to Blind Source Separation.pdf:pdf},
isbn = {1-4244-0469-X},
journal = {2006 IEEE International Conference on Acoustics Speed and Signal Processing Proceedings},
number = {1},
pages = {V--621--V--624},
publisher = {Ieee},
title = {{New Algorithms for Non-Negative Matrix Factorization in Applications to Blind Source Separation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1661352}
}
@incollection{NIPS2010_0330,
author = {Lempitsky, Victor and Zisserman, Andrew},
booktitle = {Advances in Neural Information Processing Systems 23},
editor = {Lafferty, J and Williams, C K I and Shawe-Taylor, J and Zemel, R S and Culotta, A},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Lempitsky, Zisserman - 2010 - Learning To Count Objects in Images.pdf:pdf},
pages = {1324--1332},
title = {{Learning To Count Objects in Images}},
year = {2010}
}
@article{Lee2011,
author = {Lee, Jong-Hwan and Hashimoto, Ryuichiro and Wible, Cynthia G. and Yoo, Seung-Schik},
doi = {10.1002/ima.20276},
file = {:Users/ondrejmandula/Documents/papers/20276_ftp.pdf:pdf},
issn = {08999457},
journal = {International Journal of Imaging Systems and Technology},
keywords = {a part of,ac,connectivity,correspondence to,default-mode network,e-mail,effective,factorization,fmri,functional connectivity,jong-hwan lee,jonghwan_lee,korea,kr,nonnegative matrix,resting-state networks,spontaneous brain activity},
month = jun,
number = {2},
pages = {211--222},
title = {{Investigation of spectrally coherent resting-state networks using non-negative matrix factorization for functional MRI data}},
url = {http://doi.wiley.com/10.1002/ima.20276},
volume = {21},
year = {2011}
}
@inproceedings{Canny2004,
author = {Canny, John},
booktitle = {Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Canny - 2004 - GaP A Factor Model for Discrete Data.pdf:pdf},
isbn = {1581138814},
keywords = {em algo-,latent semantic analysis,probabilistic models},
pages = {122--129},
publisher = {ACM},
title = {{GaP: a factor model for discrete data}},
url = {http://portal.acm.org/citation.cfm?id=1009016},
year = {2004}
}
@article{Buntine2002,
author = {Buntine, Wray},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Buntine - 2002 - Variational Extensions to EM and Multinomial.pdf:pdf},
journal = {Machine Learning: ECML 2002},
pages = {23--34},
publisher = {Springer},
title = {{Variational extensions to EM and multinomial PCA}},
url = {http://www.springerlink.com/index/QFUL9ALQUWLENLNT.pdf},
year = {2002}
}
@article{Hyvarinen2000,
abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of non-Gaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
author = {Hyv\"{a}rinen, a and Oja, E},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Hyv\"{a}rinen, Oja - 2000 - Independent component analysis algorithms and applications..pdf:pdf},
issn = {0893-6080},
journal = {Neural networks : the official journal of the International Neural Network Society},
keywords = {Algorithms,Artifacts,Brain,Brain: physiology,Humans,Magnetoencephalography,Neural Networks (Computer),Normal Distribution},
number = {4-5},
pages = {411--30},
pmid = {10946390},
title = {{Independent component analysis: algorithms and applications.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10946390},
volume = {13},
year = {2000}
}
@article{Brunet2004,
abstract = {We describe here the use of nonnegative matrix factorization (NMF), an algorithm based on decomposition by parts that can reduce the dimension of expression data from thousands of genes to a handful of metagenes. Coupled with a model selection mechanism, adapted to work for any stochastic clustering algorithm, NMF is an efficient method for identification of distinct molecular patterns and provides a powerful method for class discovery. We demonstrate the ability of NMF to recover meaningful biological information from cancer-related microarray data. NMF appears to have advantages over other methods such as hierarchical clustering or self-organizing maps. We found it less sensitive to a priori selection of genes or initial conditions and able to detect alternative or context-dependent patterns of gene expression in complex biological systems. This ability, similar to semantic polysemy in text, provides a general method for robust molecular pattern discovery.},
annote = {Rank K estimation for NMF. },
author = {Brunet, Jean-Philippe and Tamayo, Pablo and Golub, Todd R and Mesirov, Jill P},
doi = {10.1073/pnas.0308531101},
file = {:Users/ondrejmandula/Documents/papers/PNAS-2004-Brunet-4164-9.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Algorithms,Central Nervous System Neoplasms,Central Nervous System Neoplasms: classification,Central Nervous System Neoplasms: genetics,Computational Biology,Data Interpretation, Statistical,Leukemia,Leukemia: classification,Leukemia: genetics,Medulloblastoma,Medulloblastoma: genetics,Models, Genetic,Neoplasms,Neoplasms: classification,Neoplasms: genetics},
month = mar,
number = {12},
pages = {4164--9},
pmid = {15016911},
title = {{Metagenes and molecular pattern discovery using matrix factorization.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=384712&tool=pmcentrez&rendertype=abstract},
volume = {101},
year = {2004}
}
@techreport{Neal1993,
annote = {Variance of a varible with autocorrelation funciton and how does it affect the variance. p105 eq.6.46},
author = {Neal, R.M.},
booktitle = {Intelligence},
file = {:Users/ondrejmandula/Documents/papers/neal-review.pdf:pdf},
institution = {Citeseer},
number = {September},
title = {{Probabilistic inference using Markov chain Monte Carlo methods}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.36.9055&amp;rep=rep1&amp;type=pdf},
year = {1993}
}
@article{Beckmann2004,
abstract = {We present an integrated approach to probabilistic independent component analysis (ICA) for functional MRI (FMRI) data that allows for nonsquare mixing in the presence of Gaussian noise. In order to avoid overfitting, we employ objective estimation of the amount of Gaussian noise through Bayesian analysis of the true dimensionality of the data, i.e., the number of activation and non-Gaussian noise sources. This enables us to carry out probabilistic modeling and achieves an asymptotically unique decomposition of the data. It reduces problems of interpretation, as each final independent component is now much more likely to be due to only one physical or physiological process. We also describe other improvements to standard ICA, such as temporal prewhitening and variance normalization of timeseries, the latter being particularly useful in the context of dimensionality reduction when weak activation is present. We discuss the use of prior information about the spatiotemporal nature of the source processes, and an alternative-hypothesis testing approach for inference, using Gaussian mixture models. The performance of our approach is illustrated and evaluated on real and artificial FMRI data, and compared to the spatio-temporal accuracy of results obtained from classical ICA and GLM analyses.},
author = {Beckmann, Christian F and Smith, Stephen M},
doi = {10.1109/TMI.2003.822821},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Beckmann, Smith - 2004 - Probabilistic independent component analysis for functional magnetic resonance imaging..pdf:pdf},
issn = {0278-0062},
journal = {IEEE transactions on medical imaging},
keywords = {Algorithms,Brain,Brain: physiology,Cerebral Cortex,Cerebral Cortex: physiology,Humans,Image Enhancement,Image Enhancement: methods,Image Interpretation, Computer-Assisted,Image Interpretation, Computer-Assisted: methods,Magnetic Resonance Imaging,Magnetic Resonance Imaging: methods,Models, Neurological,Models, Statistical,Neurons,Neurons: physiology,Phantoms, Imaging,Principal Component Analysis,Reproducibility of Results,Sensitivity and Specificity,Stochastic Processes,Vision, Ocular,Vision, Ocular: physiology},
month = feb,
number = {2},
pages = {137--52},
pmid = {14964560},
title = {{Probabilistic independent component analysis for functional magnetic resonance imaging.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/14964560},
volume = {23},
year = {2004}
}
@article{Bach2003,
author = {Bach, Francis R. and Jordan, Michael I.},
doi = {10.1162/153244303768966085},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Bach, Jordan - 2003 - Kernel Independent Component Analysis.pdf:pdf},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {blind source separation,canonical correlations,gral equations,gram matrices,incomplete cholesky decomposition,independent component analysis,inte-,kernel methods,mutual information,semiparametric models,stiefel manifold},
month = jan,
number = {1},
pages = {1--48},
title = {{Kernel Independent Component Analysis}},
url = {http://www.crossref.org/jmlr_DOI.html},
volume = {3},
year = {2003}
}
@article{Titsias2007,
author = {Titsias, M},
file = {:Users/ondrejmandula/Documents/papers/nips07.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{The infinite gamma-poisson feature model}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.102.5807&amp;rep=rep1&amp;type=pdf},
year = {2007}
}
@article{McKeown2003,
author = {McKeown, M},
doi = {10.1016/j.conb.2003.09.012},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/McKeown - 2003 - Independent component analysis of functional MRI what is signal and what is noise.pdf:pdf},
issn = {09594388},
journal = {Current Opinion in Neurobiology},
month = oct,
number = {5},
pages = {620--629},
title = {{Independent component analysis of functional MRI: what is signal and what is noise?}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0959438803001338},
volume = {13},
year = {2003}
}
@article{Rao1945,
author = {Rao, C R},
journal = {Bull Calcutta Math Soc},
pages = {81--91},
title = {{Information and accuracy attainable in the estimation of statistical parameters}},
volume = {37},
year = {1945}
}
@article{Lee1999,
abstract = {Is perception of the whole based on perception of its parts? There is psychological and physiological evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
author = {Lee, D D and Seung, H S},
doi = {10.1038/44565},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Seung - 1999 - Learning the parts of objects by non-negative matrix factorization..pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Algorithms,Face,Humans,Learning,Models, Neurological,Perception,Perception: physiology,Semantics},
month = oct,
number = {6755},
pages = {788--91},
pmid = {10548103},
title = {{Learning the parts of objects by non-negative matrix factorization.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10548103},
volume = {401},
year = {1999}
}
@article{Smith2010a,
abstract = {We describe an iterative algorithm that converges to the maximum likelihood estimate of the position and intensity of a single fluorophore. Our technique efficiently computes and achieves the Cram\'{e}r-Rao lower bound, an essential tool for parameter estimation. An implementation of the algorithm on graphics processing unit hardware achieved more than 10(5) combined fits and Cram\'{e}r-Rao lower bound calculations per second, enabling real-time data analysis for super-resolution imaging and other applications.},
author = {Smith, Carlas S C.S. and Joseph, Nikolai and Rieger, Bernd and Lidke, Keith a K.A.},
doi = {10.1038/nmeth.1449},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Smith et al. - 2010 - localization that achieves theoretically minimum uncertainty.pdf:pdf;:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Smith et al. - 2010 - Fast, single-molecule localization that achieves theoretically minimum uncertainty..pdf:pdf},
issn = {1548-7091},
journal = {nature methods},
month = apr,
number = {5},
pages = {373--375},
pmid = {20364146},
publisher = {Nature Publishing Group},
title = {{Fast, single-molecule localization that achieves theoretically minimum uncertainty}},
url = {http://www.nature.com/nmeth/journal/v7/n5/abs/nmeth.1449.html http://www.ncbi.nlm.nih.gov/pubmed/20364146},
volume = {7},
year = {2010}
}
@article{Kim2008,
author = {Kim, Jingu and Park, Haesun},
doi = {10.1109/ICDM.2008.149},
file = {:Users/ondrejmandula/Documents/papers/04781130.pdf:pdf},
isbn = {978-0-7695-3502-9},
journal = {2008 Eighth IEEE International Conference on Data Mining},
keywords = {NMF},
mendeley-tags = {NMF},
month = dec,
pages = {353--362},
publisher = {Ieee},
title = {{Toward Faster Nonnegative Matrix Factorization: A New Algorithm and Comparisons}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4781130},
year = {2008}
}
@article{Lee2001,
author = {Lee, D.D. and Seung, H.S.},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Seung - 2001 - Algorithms for non-negative matrix factorization.pdf:pdf},
journal = {Advances in neural information processing systems},
publisher = {Citeseer},
title = {{Algorithms for non-negative matrix factorization}},
url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:Algorithms+for+non-negative+matrix+factorization#0},
volume = {13},
year = {2001}
}
@inproceedings{Buntine2006,
annote = {Statistical and Optimization Perspectives Workshop, SLSFS 2005 Bohinj, Slovenia, February 23-25, 2005, Revised Selected Papers},
author = {Buntine, Wray and Jakulin, Aleks},
booktitle = {Subspace, Latent Structure and Feature Selection},
editor = {Saunders, C. and Grobelnik, M. and Gunn, S. and Shawe-Taylor, J.},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Buntine, Jakulin - 2006 - LNCS 3940 - Discrete Component Analysis.pdf:pdf},
isbn = {978-3-540-34137-6},
keywords = {Bayesian inference - algorithmic learning - cluste},
pages = {1--33},
publisher = {Springer},
title = {{Discrete component analysis}},
url = {http://www.springerlink.com/index/d53027666542q3v7.pdf},
year = {2006}
}
@article{Lin2007,
abstract = {Nonnegative matrix factorization (NMF) can be formulated as a minimization problem with bound constraints. Although bound-constrained optimization has been studied extensively in both theory and practice, so far no study has formally applied its techniques to NMF. In this letter, we propose two projected gradient methods for NMF, both of which exhibit strong optimization properties. We discuss efficient implementations and demonstrate that one of the proposed methods converges faster than the popular multiplicative update approach. A simple Matlab code is also provided.},
author = {Lin, Chih-Jen},
doi = {10.1162/neco.2007.19.10.2756},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Lin - 2007 - Projected gradient methods for nonnegative matrix factorization.(2).pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Artificial Intelligence,Image Interpretation, Computer-Assisted,Image Interpretation, Computer-Assisted: methods,Least-Squares Analysis,Models, Theoretical,Pattern Recognition, Automated,Pattern Recognition, Automated: methods},
month = oct,
number = {10},
pages = {2756--79},
pmid = {17716011},
title = {{Projected gradient methods for nonnegative matrix factorization.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18188201},
volume = {19},
year = {2007}
}
@article{Wiskott2002,
abstract = {Invariant features of temporally varying signals are useful for analysis and classification. Slow feature analysis (SFA) is a new method for learning invariant or slowly varying features from a vectorial input signal. It is based on a nonlinear expansion of the input signal and application of principal component analysis to this expanded signal and its time derivative. It is guaranteed to find the optimal solution within a family of functions directly and can learn to extract a large number of decorrelated features, which are ordered by their degree of invariance. SFA can be applied hierarchically to process high-dimensional input signals and extract complex features. SFA is applied first to complex cell tuning properties based on simple cell output, including disparity and motion. Then more complicated input-output functions are learned by repeated application of SFA. Finally, a hierarchical network of SFA modules is presented as a simple model of the visual system. The same unstructured network can learn translation, size, rotation, contrast, or, to a lesser degree, illumination invariance for one-dimensional objects, depending on only the training stimulus. Surprisingly, only a few training objects suffice to achieve good generalization to new objects. The generated representation is suitable for object recognition. Performance degrades if the network is trained to learn multiple invariances simultaneously.},
author = {Wiskott, Laurenz and Sejnowski, Terrence J},
doi = {10.1162/089976602317318938},
file = {:Users/ondrejmandula/Documents/papers/Wiskott2002NC.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Artificial Intelligence,Computer Simulation,Neural Networks (Computer),Nonlinear Dynamics,Pattern Recognition, Automated,Photoreceptor Cells,Visual Perception},
month = apr,
number = {4},
pages = {715--70},
pmid = {11936959},
title = {{Slow feature analysis: unsupervised learning of invariances.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11936959},
volume = {14},
year = {2002}
}
@article{Mairal2010,
abstract = {NMF},
author = {Mairal, Julien and Bach, Francis and Ponce, J. and Sapiro, Guillermo},
file = {:Users/ondrejmandula/Documents/papers/mairal10a.pdf:pdf},
journal = {The Journal of Machine Learning Research},
keywords = {basis pursuit,dictionary learning,ing,matrix factorization,negative matrix factorization,non-,online learning,sparse cod-,sparse principal component analysis,stochastic approximations,stochastic optimization},
pages = {19--60},
publisher = {MIT Press},
title = {{Online learning for matrix factorization and sparse coding}},
url = {http://portal.acm.org/citation.cfm?id=1756008},
volume = {11},
year = {2010}
}
@article{Blei2003,
annote = {This shows that the integrating out the hidden variables (here theta) is intractable (Dickey 1983) - section 5.1},
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
doi = {10.1162/jmlr.2003.3.4-5.993},
editor = {Lafferty, John},
file = {:Users/ondrejmandula/Documents/papers/BleiNgJordan2003.pdf:pdf},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
month = may,
number = {4-5},
pages = {993--1022},
title = {{Latent Dirichlet Allocation}},
url = {http://www.crossref.org/jmlr_DOI.html},
volume = {3},
year = {2003}
}
@inproceedings{Kolenda2001,
annote = {Molgedey, Schuster, extended for non-square matrices. Implementation in matlab.
Some notes on how to determine Tau for which the autocorrelations differs the most.},
author = {Kolenda, Thomas and Hansen, L.K. and Larsen, Jan},
booktitle = {Third International Conference on Independent Component Analysis and Blind Source Separation},
file = {:Users/ondrejmandula/Documents/papers/koleda2001ica.pdf:pdf},
number = {1},
pages = {540--545},
publisher = {Citeseer},
title = {{Signal detection using ICA: Application to chat room topic spotting}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.29.3705&amp;rep=rep1&amp;type=pdf},
year = {2001}
}
@article{Liu2006,
author = {Liu, Weixiang and Zheng, Nanning and You, Qubo},
doi = {10.1007/s11434-005-1109-6},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Zheng, You - 2006 - Nonnegative matrix factorization and its applications in pattern recognition(2).pdf:pdf},
issn = {1001-6538},
journal = {Chinese Science Bulletin},
keywords = {1,detection,digital watermarking,eeg signal analysis,feature extraction,fields,in numerical algebra,intrusion,matrix factorization has been,nmf,nonnegative data,the large-scale and com-,widely used in many},
month = jan,
number = {1},
pages = {7--18},
title = {{Nonnegative matrix factorization and its applications in pattern recognition}},
url = {http://www.springerlink.com/index/10.1007/s11434-005-1109-6},
volume = {51},
year = {2006}
}
@article{Bailey1995,
author = {Bailey, Timothy L.},
doi = {10.1007/BF00993379},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Bailey - 1995 - Unsupervised learning of multiple motifs in biopolymers using expectation maximization.pdf:pdf},
journal = {Machine Learning},
number = {2},
pages = {6727--80},
title = {{Unsupervised learning of multiple motifs in biopolymers using expectation maximization}},
volume = {12},
year = {1995}
}
@article{Frigyesi2008,
abstract = {Non-negative matrix factorization (NMF) is a relatively new approach to analyze gene expression data that models data by additive combinations of non-negative basis vectors (metagenes). The non-negativity constraint makes sense biologically as genes may either be expressed or not, but never show negative expression. We applied NMF to five different microarray data sets. We estimated the appropriate number metagens by comparing the residual error of NMF reconstruction of data to that of NMF reconstruction of permutated data, thus finding when a given solution contained more information than noise. This analysis also revealed that NMF could not factorize one of the data sets in a meaningful way. We used GO categories and pre defined gene sets to evaluate the biological significance of the obtained metagenes. By analyses of metagenes specific for the same GO-categories we could show that individual metagenes activated different aspects of the same biological processes. Several of the obtained metagenes correlated with tumor subtypes and tumors with characteristic chromosomal translocations, indicating that metagenes may correspond to specific disease entities. Hence, NMF extracts biological relevant structures of microarray expression data and may thus contribute to a deeper understanding of tumor behavior.},
author = {Frigyesi, Attila and H\"{o}glund, Mattias},
file = {:Users/ondrejmandula/Documents/papers/cin-6-0275.pdf:pdf},
issn = {1176-9351},
journal = {Cancer informatics},
keywords = {gene expression,metagenes,nmf,tumor classifi cation},
month = jan,
number = {2003},
pages = {275--92},
pmid = {19259414},
title = {{Non-negative matrix factorization for the analysis of complex gene expression data: identification of clinically relevant tumor subtypes.}},
url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2623306/ http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2623306&tool=pmcentrez&rendertype=abstract},
volume = {6},
year = {2008}
}
@article{Hansen2000,
annote = {Molgedey, Schuster, extended for non-square matrices.
Does not take the noise into account. },
author = {Hansen, L.K. and Larsen, J. and Kolenda, T.},
file = {:Users/ondrejmandula/Library/Application Support/Mendeley Desktop/Downloaded/Hansen, Larsen, Kolenda - 2000 - On independent component analysis for multimedia signals.pdf:pdf},
journal = {Multimedia Image and Video Processing},
keywords = {Molgedey,Schuster,correlations},
pages = {175--199},
publisher = {CRC$\$ Press},
title = {{On independent component analysis for multimedia signals}},
url = {http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:On+Independent+Component+Analysis+for+Multimedia+Signals#0},
year = {2000}
}
